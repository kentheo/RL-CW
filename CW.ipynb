{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,2),(2,0),(3,0),(3,1),(3,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,1),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def policy_improvement(self, value_function, discount):\n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        V = np.zeros(value_function.shape[0])\n",
    "        \n",
    "        policy = np.zeros((self.state_size, self.action_size))\n",
    "        \n",
    "        for state_idx in range(value_function.shape[0]):\n",
    "            # If it is one of the absorbing states, ignore\n",
    "            if(self.absorbing[0,state_idx]):\n",
    "                continue   \n",
    "\n",
    "            # Array for Q of actions\n",
    "            q_a = np.zeros(self.action_size)\n",
    "            \n",
    "            for action_idx in range(self.action_size):\n",
    "                # Accumulator variable for the State-Action Value\n",
    "                tmpQ = 0\n",
    "                for state_idx_prime in range(policy.shape[0]):\n",
    "                    tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                q_a[action_idx] = tmpQ\n",
    "            \n",
    "            policy[state_idx] = np.argmax(q_a)\n",
    "        return policy\n",
    "    \n",
    "    def policy_iteration_algorithm(self, discount):\n",
    "        policy = np.zeros((self.state_size, self.action_size))\n",
    "#         print(\"First random policy = \", policy.shape)\n",
    "        \n",
    "        iterations = 1000\n",
    "        threshold = 0.001\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            prev_policy = self.policy_evaluation(policy, threshold, discount)\n",
    "#             print(\"Previous policy shape = \", prev_policy.shape)\n",
    "            next_policy = self.policy_improvement(prev_policy, discount)\n",
    "#             print(\"Next policy shape = \", next_policy.shape)\n",
    "            if (np.all(policy == next_policy)):\n",
    "                print ('Policy Iteration Algorithm converged at step %d.' %(i+1))\n",
    "                break\n",
    "            policy = next_policy\n",
    "        return policy\n",
    "            \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 11 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the location of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                # And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grid and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "The value of that policy is : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "Policy= np.zeros((grid.state_size, grid.action_size))\n",
    "# Policy1 = np.array([0.0, 1.0, 0.0, 0.0])\n",
    "print(\"The Policy is : {}\".format(Policy))\n",
    "\n",
    "threshold = 0.1 \n",
    "discount = 0.9\n",
    "\n",
    "val = grid.policy_evaluation(Policy, threshold, discount) #Change here!\n",
    "print(\"The value of that policy is : {}\".format(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(11).astype(int)\n",
    "print(Policy2.shape)\n",
    "Policy2[2] = 3\n",
    "Policy2[6] = 2\n",
    "Policy2[10] = 1\n",
    "grid.draw_deterministic_policy(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAACFCAYAAAB7VhJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAACC5JREFUeJzt3c9rHPcdxvHnqazYbRwwUQSxLVG30BRCCU4R7sE3hyI3h6bHupBTqU+BBHrJqdD+Ab314tLgHEJDIDmEkiJCMZSE1rFi1FDbTTCmxUpS4siIxDH+JX96kDCqLdCsPN+Z/Xz3/YIFSV5/9zP7iMfj2dlZR4QAAHl8re8BAACDobgBIBmKGwCSobgBIBmKGwCSobgBIBmKGwCSobgBIBmKGwCS2VZi0Qe8PXbowRJLYwDX9JVuxHW3td4jD4/Fvunxtpbb0EcffKPo+o89cbXo+l3498Wb+vzySmu57np4LB6dKlIFd+x0a+Nu6EoF7wD/7+ItLTfMtUhaO/SgfuCnSiyNAZyMv7S63r7pcb03N93qmneb3bO/6PpzcwtF1+/CgdmLra736NQ2/f7NqVbXvNvBHWX/c//utdtF1+/CL3682Pi+HCoBgGQobgBIhuIGgGQobgBIhuIGgGQobgBIhuIGgGQaFbftw7Y/tH3e9oulh0I3yLVO5Fq/TYvb9pik30n6kaTHJR2x/XjpwVAWudaJXEdDkz3uA5LOR8SFiLgh6VVJz5QdCx0g1zqR6whoUtx7Ja1/j+3i2s+QG7nWiVxHQGsvTto+anve9vxNXW9rWfRsfa6Xllb6HgctWZ/r8lL+63yMmibF/bGk9VcWmlr72f+JiGMRMRMRM+Pa3tZ8KGfgXCcnxjobDls2cK67Jji5LJsmiZ2S9B3b37L9gKSfSnqz7FjoALnWiVxHwKaXdY2IW7afkzQnaUzSSxFxpvhkKIpc60Suo6HR9bgj4i1JbxWeBR0j1zqRa/04uAUAyVDcAJAMxQ0AyVDcAJAMxQ0AyVDcAJAMxQ0AyTQ6j3sUzX2yUPwxZvfsL/4Y2XTxvKN7717jeihtYo8bAJKhuAEgGYobAJKhuAEgGYobAJKhuAEgGYobAJKhuAEgmU2L2/ZLtj+z/c8uBkI3yLVeZFu/JnvcxyUdLjwHundc5Fqr4yLbqm1a3BHxV0mXO5gFHSLXepFt/Vo7xm37qO152/M3db2tZdGz9bleWlrpexy0ZH2uy0tcRySb1oo7Io5FxExEzIxre1vLomfrc52cGOt7HLRkfa67JjhHIRsSA4BkKG4ASKbJ6YB/lPQ3Sd+1vWj75+XHQmnkWi+yrd+mH6QQEUe6GATdItd6kW39OFQCAMlQ3ACQDMUNAMlQ3ACQDMUNAMlQ3ACQzKanA27FY09c1dzcQoml75jdsz/1+gCwVexxA0AyFDcAJENxA0AyFDcAJENxA0AyFDcAJENxA0AyFDcAJNPkgxSmbZ+wfdb2GdvPdzEYyiLXOpHraGjyzslbkn4ZEadtPyTpfdtvR8TZwrOhLHKtE7mOgE33uCPi04g4vfb1l5LOSdpbejCURa51ItfRMNAxbtv7JD0p6WSJYdAPcq0TudarcXHb3inpdUkvRMQXG/z5UdvztucvLa20OSMKItc6DZLr8tLt7gfEfWlU3LbHtfpL8EpEvLHRfSLiWETMRMTM5MRYmzOiEHKt06C57prg5LJsmpxVYkl/kHQuIn5bfiR0gVzrRK6jock/tQclPSvpkO2FtdvThedCeeRaJ3IdAZueDhgR70hyB7OgQ+RaJ3IdDRzcAoBkKG4ASIbiBoBkKG4ASIbiBoBkKG4ASIbiBoBkmlzWdSjNfbJQdP3ZPfuLri+V34YDs1eLrp9RDbniXr/59veLP8avLpwu/hhNsccNAMlQ3ACQDMUNAMlQ3ACQDMUNAMlQ3ACQDMUNAMlQ3ACQTJOPLtth+z3b/7B9xvavuxgMZZFrnch1NDR55+R1SYci4srah5C+Y/vPEfH3wrOhLHKtE7mOgCYfXRaSrqx9O752i5JDoTxyrRO5joZGx7htj9lekPSZpLcj4uQG9zlqe972/KWllbbnRAHkWqdBc11eut39kLgvjYo7IlYiYr+kKUkHbH9vg/sci4iZiJiZnBhre04UQK51GjTXXROco5DNQIlFxLKkE5IOlxkHfSDXOpFrvZqcVTJpe9fa11+X9ENJ/yo9GMoi1zqR62hoclbJbkkv2x7TatG/FhF/KjsWOkCudSLXEdDkrJIPJD3ZwSzoELnWiVxHA69KAEAyFDcAJENxA0AyFDcAJENxA0AyFDcAJENxA0AyXr2YWMuL2pck/WeAv/KIpM9bH6Rbw7gN34yIybYWI9ehQa73bxi3oXGuRYp7ULbnI2Km7znuRw3b0LYanpMatqFtNTwn2beBQyUAkAzFDQDJDEtxH+t7gBbUsA1tq+E5qWEb2lbDc5J6G4biGDcAoLlh2eMGADTUa3HbPmz7Q9vnbb/Y5yxbZXva9gnbZ22fsf183zMNg+zZkuvGyHU49HaoZO1C7x9p9RM6FiWdknQkIs72MtAW2d4taXdEnLb9kKT3Jf0k23a0qYZsyfVe5Do8+tzjPiDpfERciIgbkl6V9EyP82xJRHwaEafXvv5S0jlJe/udqnfpsyXXDZHrkOizuPdKurju+0UlfALXs71Pq58+crLfSXpXVbbkege5DglenGyJ7Z2SXpf0QkR80fc8aAe51il7rn0W98eSptd9P7X2s3Rsj2v1l+CViHij73mGQBXZkus9yHVI9Pni5DatvtDxlFbDPyXpZxFxppeBtsi2Jb0s6XJEvND3PMOghmzJ9V7kOjx62+OOiFuSnpM0p9UXCF7L9AuwzkFJz0o6ZHth7fZ030P1qZJsyfUu5Do8eOckACTDi5MAkAzFDQDJUNwAkAzFDQDJUNwAkAzFDQDJUNwAkAzFDQDJ/A/cB++S+uupxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First random policy =  (11, 4)\n",
      "Policy Iteration Algorithm converged at step 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/kt3118/Desktop/RL/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:124: RuntimeWarning: overflow encountered in double_scalars\n",
      "/homes/kt3118/Desktop/RL/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/homes/kt3118/Desktop/RL/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:124: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/homes/kt3118/Desktop/RL/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:130: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-0039c2f1aa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimal_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_iteration_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_deterministic_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-aefe84a13b16>\u001b[0m in \u001b[0;36mdraw_deterministic_policy\u001b[0;34m(self, Policy)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0marrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mr\"$\\uparrow$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr\"$\\rightarrow$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"$\\downarrow$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"$\\leftarrow$\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0maction_arrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_arrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "discount = 0.75\n",
    "\n",
    "optimal_policy = grid.policy_iteration_algorithm(discount).astype(int)\n",
    "grid.draw_deterministic_policy(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
